---
title: "任务一-线性回归算法梳理"
categories: "初级算法梳理"
tags:
  - ML
comments: true
date: 2019-03-01 21:44:03
---

# 机器学习的一些概念

有监督、无监督、泛化能力、过拟合欠拟合(方差和偏差以及各自解决办法)、交叉验证

* 有监督学习：数据集有有已知的y值（label 结果）

* 无监督学习：数据集中没有y值，需要根据近似性关系分成一簇一簇的聚类来当作y值来评估

* 泛化能力：指算法对具有同一规律的学习集以外的数据，的适用程度，对其它样本的适应能力

<!--more-->

## 过拟合

over-fitting，指模型在训练样本中表现过好，要求过于精细，导致泛化能力减弱，在验证数据集和测试集中表现不佳。用图表示就是曲线起伏过大，不平稳。也称高方差。

### 解决办法

过拟合可以通过减少参数，加入正则惩罚项 L1(绝对值项) L2，变化学习率，尽可能扩大数据集选取范围等方式解决。

还要注意训练模型如果出现测试精度超1000次以上都在持续下降，而训练集精度较高时，要及时停止训练

## 欠拟合

under-fitting，与上相反，指模型过于简单或训练样本做的不够，比如特征过省，导致的模型在验证集测试集数据中表现不好，没有代表性。用图表现就是一条无起伏的线。也称高偏差。

### 解决办法

欠拟合可以通过交叉验证，让特征较少的情况多次迭代交替使用训练集和验证集，达到优化，或跟据相关性添加其它特征项，减少正则化参数。神经网络可以加结点加层数。

增加训练迭代，加数据，加节点，加层数（3层以内）

## 方差和偏差的概念扩展：

算法的预测误差可以分解为三个部分: 偏差(bias)， 方差(variance) 和噪声(noise).

* 偏差：指偏离真实label导致的误差，反映了学习算法本身的拟合能力
* 方差：随机变量在其期望值附近的波动程度，即预测值在真实值附近的波动，如同样大小的训练集的变动所导致的学习性能的变化。它反映了数据扰动所造成的影响。
* 噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界， 即 刻画了学习问题本身的难度 . 巧妇难为无米之炊， 给一堆很差的食材， 要想做出一顿美味， 肯定是很有难度的.

* 交叉验证：把特征分成几部分：一些作为训练集一些做验证集，下一次交换角色，用验证集数据做训练集，训练集做验证集，交替多次充份训练验证数据。

# 线性回归的原理

建立模型，通过输入特征项加误差项，找到最合适的最好拟合结果y值的数据点。
（误差项是独立并有相同分布，服从均值为0方法为θ^2的高斯分布正态分布，即符合中心极限定理。）

# 线性回归损失函数、代价函数、目标函数

* 损失函数：模型的样本误差，用于度量预测值与真实值的拟合程度，损失函数越小说明模型越好。它是定义在单个样本上的。
* 代价函数：定义在整个训练集上的，整个样本误差的平均，即，损失函数的平均值。
* 目标函数：表示误差的损失函数，代价函数，加入了正则惩罚项后得到的最终优化函数。

# 优化方法(梯度下降法、牛顿法、拟牛顿法等)

* 梯度下降法：通过一步一步迭代，边训练数据，边调整参数，计算偏导，使回归使终是保持梯度下降的，即最优，来得到最小化的损失函数和此时的模型参数值

* 牛顿法：在梯度下降原理基础上，优化的二阶收敛，下降过程中采用二次曲面，考虑了每走一步对当前最大角度产生的影响，梯度下降是一阶收敛，一个平面下静态的拟合局部，只考虑了本步静态的最大方向。
所以牛顿法比梯度下降法下降要快。

* 拟牛顿法：在每一步迭代时只要求知道目标函数梯度，通过测量梯度变化构造一个目标函数的模型，使之产生超线性收敛性。不需要二阶层数信息。可以用来解决无约事，约事，和大规模的优化问题。
它是解非线性方程组及最优化计算中最有效的方法之一。
是一类使每步迭代计算量少而又保持超线性收敛的牛顿型迭代法。

# 线性回归的评估指标

* SSE(误差平方和)：误差平方和，即(真实值-预测值)的平方和
同样的数据集的情况下，SSE越小，误差越小，模型效果越好
缺点：随着样本增加，SSE必然增加，也就是说，不同的数据集的情况下，SSE比较没有意义

* R-square(决定系数)：1- 误差平方和/原始真实数据与平均值差方和。综合考虑了预测数据和原始数据的误差以及原始数据的离散程度。消除了原始数据离散程度的影响。
R^2 越接近1，表示方程变量对y的解释能力越强，模型对数据拟合越好。
R^2 越接近0，表明模型拟合越差。
缺点：数据集的样本越大，R^2越大，所以，不同数据集的模型结果比较会有一定的误差

* Adjusted R-square：校正决定系数。加入样本数据和特征数据评价指标，消除了样本数据和特征数量的影响。

# sklearn 参数详解

sklearn 线性回归LinearRegression()参数：

| 参数 | 意义 |
|:---:|---|
| fit_intercept | 是否有截据，如果没有则直线过原点 |
| normalize | 是否将数据归一化 |
| copy_X | 默认为True，当为True时，X会被copied，否则X将会被覆写 |
| n_jobs | 默认值为1，计算时使用的核数 |

Sklearn 逻辑回归LogisticRegression()参数:

| 参数 | 意义 |
|:---:|---|
| penalty | 正则惩罚项，值为L1 L2，默认L2 |
| random_state | 随机种子，设置为0则，保证每次随机生成的数据比例一致 |
| C | 正则强度，较小的值指定更强的正则化。默认为1 |