---
title: "week4-3 Discrimination / Bias 歧视/偏见"
categories: "AI For Everyone"
tags:
  - AI
  - DL
  - ML
comments: true
date: 2019-03-04 11:27:20
---

![4-3-0.png](https://upload-images.jianshu.io/upload_images/910914-ec9204bf47adf75e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

人工智能系统如何变得偏向并因此歧视某些人？我们如何在人工智能系统中尝试减少或消除这种影响？让我们从示例开始。

微软的一个小组发现了这个非凡的结果，当人工智能从互联网上的文本文件中学习时，它可以学习健康的刻板印象。

值得赞扬的是，他们还提出了减少此类AI系统偏差量的技术解决方案。

<!--more-->

![4-3-1.png](https://upload-images.jianshu.io/upload_images/910914-6c8a049cee3a42ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![4-3-2.png](https://upload-images.jianshu.io/upload_images/910914-fdc7bd49b37c8a48.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![4-3-3.png](https://upload-images.jianshu.io/upload_images/910914-d2ebf29422acdad5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这是他们发现的。

通过在互联网上阅读AI文本，它可以学习单词，你可以问它关于类比的理由。

那么，既然你已经在互联网上阅读了所有这些文字，那么你可以对AI系统进行测验。在类比中，男人就像父亲那样是什么？所以，AI会输出单词母，这反映了这些单词通常用于的方式互联网。

如果你问男人是女人，那么国王就是什么呢？那么同样的人工智能系统会说，就像国王对女王一样，相对于这些词在互联网上的使用方式而言，这似乎是合理的。

研究人员还发现了以下结果，即如果你问它，男人是计算机程序女人是什么？同样的AI系统会输出答案，女人是家庭主妇。

我觉得这个答案真的很不幸。

较少偏见的答案可言之，女人是computer programmer。

如果我们希望我们的AI系统能够理解男人和女人同样可以成为computer programmer，就像男人和女人一样是家庭主妇，那么我们希望它输出男人是computer programmer，因为女人是computer programmer，而且男人也是家庭主妇是家庭主妇。

人工智能系统如何从数据中学会变成这样的偏见？让我们更多地了解技术细节。

AI系统存储单词的方式是使用一组数字。

所以，让我们说存储 word man，或者我们有时会说代表两个数字1,1。

AI系统提供这些数字的方式是通过统计数据如何在互联网上使用 word man。

计算这些数字的具体过程非常复杂，我不会在这里讨论。

但这些数字代表了这些词的典型用法。

在实践中，AI可能有数百或数千个数字来存储单词，但我只是在这里使用两个数字来保持示例更简单。

让我把这个数字和它绘制在图表上。

所以，man这个词，我将在右边的图上的位置1,1上绘图。

通过查看短语computer programmer在互联网上如何使用或如何使用的统计数据，AI将有一对不同的数字，例如3,2，用于存储或表示短语computer programmer。

同样地，通过观察使用女性这个词，它会出现一对不同的数字，比如2,3，用来存储或代表女性这个词。

当你要求人工智能系统对上面的类比进行计算时，男人是computer programmer，因为女人要做什么？那么AI系统将做什么，就是构建一个看起来像这样的平行四边形。

它会问，与位置4相关的单词是什么？因为它会认为这是这个类比的答案。

在数学上思考这个问题的一种方法是，人工智能认为人与computer programmer的关系是你从男人这个词开始，向右走两步，然后向上走一步。

那么，为女性找到相同的答案是什么，？你也会向右两步，一步到位。

不幸的是，当这些数字来源于互联网上的文本时，AI系统会发现在互联网上使用单词homemaker的方式导致它被放置到位置4,4，这就是AIsystem提出这种偏见类比的原因。

人工智能系统今天已经做出了重要的决定，并且将来也将继续发挥作用。

所以，偏见很重要。

例如，有一家公司使用人工智能进行招聘，并发现他们的招聘过于歧视女性。

这显然是不公平的，所以这个公司就是他们的工具。

其次，还有一些面部识别系统，对于浅肤色和皮肤黝黑的人来说似乎更准确。

如果AI系统主要是根据打火机个体的数据进行训练，那么对于该类别的个体而言，如果将这些系统用于例如犯罪调查，则这将更加准确，这可能对黑皮肤的个体产生非常有偏见和不公平的影响。

因此，今天许多面部识别团队正在努力确保系统不会出现这种偏见。

还有人工智能或统计数据审批制度最终歧视一些少数民族群体，并引用了更高的利率。

银行也一直致力于确保在其审批系统中减少或消除这种偏见。

最后，我认为人工智能系统对加强健康刻板印象的毒性作用没有贡献是很重要的。

例如，ifan-8岁的女孩去图像搜索引擎并搜索首席执行官，如果他们只看到男人的照片，或者他们看到没有人看起来像是按性别或种族划分，我们不希望他们成为不鼓励追求一个可能导致她有朝一日成为一家大公司的首席执行官的职业。

由于存在这些问题，人工智能社区已经投入了大量精力来打击偏见。

例如，我们正在寻求更好和更好的解决方案，以减少AI系统的偏差。

在这个例子中你看到了AI输出买家类比的视频。

研究人员已经发现，当人工智能系统学习了很多不同的数字来存储单词时，很少有数字与偏差相对应。

如果将这些数字归零，只需将它们设置为零，那么偏差就会显着减小。

第二种解决方案是尝试使用较少偏见和更具包容性的数据。

例如，如果您正在建立一个面部识别系统，并确保包含来自多个种族和所有性别的数据，那么您的系统将更少偏见和更具包容性。

其次，许多人工智能团队使他们的系统更加透明和/或审核过程，这样我们就可以不断检查这些人工智能系统所展示的偏见（如果有的话），这样我们至少可以认识到它存在的问题，如果它存在，然后采取措施解决它。

例如，许多面部识别团队正在系统地检查他们的系统在人口的不同子集上的准确程度，以检查，例如，对于皮肤较暗的对比度较高的个体，它是否更准确或更不准确。

拥有透明的系统以及系统的审计流程会增加至少能够快速发现问题的几率，以防有问题，以便我们可以解决它。

最后，我认为拥有多元化的员工队伍有助于减少偏见。

如果您拥有多元化的员工队伍，那么您的员工中的个人更有可能能够解决不同的问题，并且他们可能会帮助您使数据更加多样化和更具包容性。

通过在建立AI系统时拥有更多独特的观点，我认为我们所有人都希望创建更少的偏见应用程序。

人工智能系统今天做出了非常重要的决定，所以偏见或潜在偏见是我们必须注意并努力减少的。

令人乐观的一件事是，我们今天实际上有更好的想法来减少AI的偏见，而不是减少人类的偏见。

因此，虽然我们不应该满意，直到所有人工智能的偏见都消失了，并且我们需要完成工作才能实现目标，我也很乐观地认为我们可以采用与人类相似的人工智能系统，因为它学到了来自人类，我们可以通过技术解决方案或其他方式减少来自那里的偏见，这样，作为一个社会，我们可以希望通过人类或通过人工智能做出的决定迅速变得更加公平和偏见。

除了偏见问题之外，人工智能的另一个限制是它可能是开放的对抗性攻击。

在下一个视频中，您将了解什么是对抗性攻击，以及您可以采取哪些措施来防范这些攻击。

让我们继续下一个视频。

讲师：[Andrew Ng](https://www.coursera.org/instructor/andrewng)
课程：<https://www.coursera.org/learn/ai-for-everyone>
