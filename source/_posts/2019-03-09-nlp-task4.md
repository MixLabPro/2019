---
title: "任务四-传统机器学习"
categories: "NLP实践"
tags:
  - ML
  - NLP
comments: true
date: 2019-03-09 20:07:22
---

# 朴素贝叶斯的原理

基于朴素贝叶斯公式，比较出后验概率的最大值来进行分类，后验概率的计算是由先验概率与类条件概率的乘积得出，先验概率和类条件概率要通过训练数据集得出，即为朴素贝叶斯分类模型，将其保存为中间结果，测试文档进行分类时调用这个中间结果得出后验概率。

<!--more-->

# 利用朴素贝叶斯模型进行文本分类

# SVM的原理

SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。如下图所示

![https://zh.wikipedia.org/wiki/支持向量机](https://upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/440px-Svm_max_sep_hyperplane_with_margin.png)

$$
\boldsymbol{w} \cdot x+b=0
$$

即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。

# 利用SVM模型进行文本分类

# pLSA、共轭先验分布；LDA主题模型原理

LDA的算法是基于Gibbs Sampling算法的，Gibbs Sampling算法又是基于MCMC的。LDA的目的是获得满足w和z的联合分布的样本点（词的主题）。而Gibbs Sampling就是通过迭代某个维度的条件概率（每个维度对应某个文档的某个位置的词）获得平稳状态，而这平稳状态的分布即这条件概率对应的联合概率。所以我们可以通过这种方法，得到稳定分布的样本点。

# 使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类

# 参考

朴素贝叶斯1：[sklearn：朴素贝叶斯（naïve beyes） - 专注计算机体系结构 - CSDN博客](https://blog.csdn.net/u013710265/article/details/72780520)

LDA数学八卦

lda2：[用LDA处理文本(Python) - 专注计算机体系结构 - CSDN博客](https://blog.csdn.net/u013710265/article/details/73480332)

合并特征：[Python：合并两个numpy矩阵 - 专注计算机体系结构 - CSDN博客](https://blog.csdn.net/u013710265/article/details/72848564)
