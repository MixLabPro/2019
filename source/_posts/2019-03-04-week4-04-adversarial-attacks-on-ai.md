---
title: "week4-4 Adversarial attacks on AI 对人工智能的对抗性攻击"
categories: "AI For Everyone"
tags:
  - AI
  - DL
  - ML
comments: true
date: 2019-03-04 11:27:21
---

![4-4-0.png](https://upload-images.jianshu.io/upload_images/910914-67ce0ae48e1bc63b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

尽管现代AI具有令人难以置信的强大功能，但现代AI技术的局限性之一，尤其是深度学习，有时候它可能会被愚弄。

特别是，现代人工智能系统有时会受到对抗性攻击，如果其他人故意故意将人工智能系统用于人工智能系统。

让我们来看看。

<!--more-->

![4-4-1.png](https://upload-images.jianshu.io/upload_images/910914-52d713121619d459.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![4-4-2.png](https://upload-images.jianshu.io/upload_images/910914-ef87d1beadd2b1f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![4-4-3.png](https://upload-images.jianshu.io/upload_images/910914-4419150315eeb454.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![4-4-4.png](https://upload-images.jianshu.io/upload_images/910914-d90b8991f58a7def.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![4-4-5.png](https://upload-images.jianshu.io/upload_images/910914-03ead716414bccf5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

让我们给你一个人工智能系统，这张鸟的图片并将其分类。

AI系统输出这是一只蜂鸟。

但是让我们对这张图片做一个小的微观。

通过微小的扰动，我的意思是稍微改变像素值，并且对大多数人来说几乎难以察觉。

相同的AI系统然后说这是锤子。

现在，对于你可能会说，“这怎么可能，右边的图片看起来几乎与左边的图片相同？”事实上，这些变化几乎让人眼不易察觉。

但是人工智能系统对你和我的看法与世界截然不同。

因此，如果对手对你和我不可察觉的图片做出改变，那么它很容易受到影响，但是他们愚弄人们认为这张照片有点不同。

我们称之为人工智能系统的对抗性攻击。

在计算机安全性中，对安全系统的攻击意味着尝试使其执行除了预期之外的其他操作。

同样地，AI系统上的对抗性攻击试图做出除了它想要做的事情以外的其他事情，例如试图欺骗它输出不正确的分类。

这是另一个例子。

这是一张野兔的照片，只是轻微的扰动或像素值的微小变化，而AI则说这是一张桌子。

计算机看到图片与人类不同的事实，给它带来了优点和缺点。

例如，在阅读条形码和QR码时，计算机系统比你和我好得多。

但是，深度学习系统工作的方式也让我们接触到这些特殊形式的攻击，这些攻击都不会被人类所愚弄。

今天，人工智能被用来过滤掉垃圾邮件，试图过滤掉仇恨言论，而攻击这些过滤器会降低这些过滤器的效率。

现在，对此幻灯片的攻击需要能够直接对图像进行修改。

例如，垃圾邮件发送者可能会在尝试将图像上传到网站或通过电子邮件发送之前直接修改图像。

还有一些攻击通过改变物理世界来发挥作用。

例如，Carnegie Mellon大学的一个小组能够设计出像这样的时髦眼镜。

因此，当他戴上这副眼镜的时候，他可以欺骗人工智能系统，让他觉得他是Mact Jovovich。

我认为戴上这样一副眼镜可以欺骗AI系统让人觉得这个男人是众所周知的女演员。

来自加州大学伯克利分校，密歇根大学和其他大学的另一组研究人员表示，如果你影响像这样的贴纸停止标志，你可以欺骗AI系统根本看不到停车标志。

除了停车标志之外还有别的东西。

关于这个例子的一个有趣的事情是它看起来像停车标志只是在它上面涂了一些涂鸦。

大多数人仍然会很容易将此视为停车标志。

但是，如果你有一个内置于自动驾驶车中的计算机视觉系统，那么如果汽车不再停止停车标志将是非常不幸的，因为这些贴纸应用在它上面。

最后一个例子，这是来自Google的一组研究人员，如果你向人工智能系统展示这张图片，它会说这是一个香蕉。

但是研究人员在设计贴纸的过程中，如果你放置到场景中，它会错误地分类这个香蕉。

让我告诉你研究人员制作的录像带。

左边显示的是分类器输入，右边显示的是分类器输出，它认为很可能是香蕉，也许是一个很小的机会。

看起来没问题。

让我们看看当你贴上贴纸或在场景中放一个小补丁时会发生什么。

当贴纸放置在场景中时，AI系统现在几乎可以肯定这张照片是烤面包机的照片。

这项工作的一个有趣的方面是本文的作者在本幻灯片的底部引用，实际上在他们的论文中发表了他们贴纸的图片。

所以，世界上的任何人都可以假设地下载他们的纸张，打印出贴纸，如果他们想要欺骗AI系统认为那里有一个没有的烤面包机，就把它贴在某处。

现在，我不支持任何人攻击AI系统来欺骗他们认为烤面包机在那里或没有，但不幸的是，这显示了人们可以轻易地假装这些AI系统。

我们可以做些什么来抵御这些对抗性攻击？幸运的是，人工智能世界一直致力于开发新技术，以便更难以攻击。

防御往往非常技术性，但有一些方法可以修改神经网络和其他人工智能系统，以使攻击更加困难。

一个缺点是这些缺陷确实会产生一些成本。

例如，AI系统可能运行得慢一点。

但这是一个正在进行研究的领域，我们远没有那些对我们想要应用AI的所有重要应用程序看起来足够好的重型对抗性攻击。

对于许多人工智能系统，可能没有任何激励试图攻击它。

例如，如果您正在运行自动视觉检测系统，以检查咖啡杯是否在您的工厂中有划痕。

也许不是那些有动力试图欺骗你的系统去思考刮刮咖啡杯的人们也没有刮擦它。

但是，人工智能应用程序也会受到攻击。

对于那些应用程序，我认为类似于垃圾邮件与反垃圾邮件，垃圾邮件发送者正在尝试通过垃圾邮件过滤器和垃圾邮件过滤器试图阻止它们。

我认为我们将在军备竞赛中应用，因为人工智能社区正在加强防御，并且有一个攻击行为的社区来愚弄我们的防御。

在我构建人工智能系统的经验中，我觉得自己与其他人完全交战的次数之一就是当我领导反欺诈团队对抗部队时。

不幸的是，互联网上存在大量欺诈行为，人们试图窃取金钱和支付系统或创建欺诈性账户。

我在反欺诈系统上工作的次数是它真正感觉像零和游戏的几次之一，有一个对手，我们会提出防御，他们会做出反应。

他们会发动攻击，我的团队必须做出反应。

有时甚至几个小时来保护自己。

因此，我认为在接下来的几年中，即使在技术发展过程中，也会出现像垃圾邮件这样的垂直行业，就像欺诈行为将会发生战争一样。

在我对对手的零和游戏中感觉如此。

话虽如此，我也不想过度夸大对象AI系统的潜在损害。

这对某些应用程序非常重要。

但也有大量的应用程序不太可能受到对抗性攻击。

现在，除了对抗性攻击外，不幸的是，AI还可以用于一些不利的负面用例。

让我们在下一个视频中看看它们中的一些以及我们应该解决它们的问题。

让我们继续下一个视频。

讲师：[Andrew Ng](https://www.coursera.org/instructor/andrewng)
课程：<https://www.coursera.org/learn/ai-for-everyone>
