---
title: "任务九-Attention原理"
categories: "NLP实践"
tags:
  - ML
  - NLP
comments: true
date: 2019-03-19 18:07:14
---

# Attention

## 为什么需要Attention

最基本的seq2seq模型包含一个encoder和一个decoder，通常的做法是将一个输入的句子编码成一个固定大小的state，然后作为decoder的初始状态（当然也可以作为每一时刻的输入），但这样的一个状态对于decoder中的所有时刻都是一样的。 

attention即为注意力，人脑在对于的不同部分的注意力是不同的。需要attention的原因是非常直观的，比如，我们期末考试的时候，我们需要老师划重点，划重点的目的就是为了尽量将我们的attention放在这部分的内容上，以期用最少的付出获取尽可能高的分数；再比如我们到一个新的班级，吸引我们attention的是不是颜值比较高的人？普通的模型可以看成所有部分的attention都是一样的，而这里的attention-based model对于不同的部分，重要的程度则不同。

<!--more-->

## Attention-based Model是什么

Attention-based Model其实就是一个相似性的度量，当前的输入与目标状态越相似，那么在当前的输入的权重就会越大，说明当前的输出越依赖于当前的输入。严格来说，Attention并算不上是一种新的model，而仅仅是在以往的模型中加入attention的思想，所以Attention-based Model或者Attention Mechanism是比较合理的叫法，而非Attention Model。

## 基本的Attention原理

对于机器翻译来说，比如我们翻译“机器学习”,在翻译“machine”的时候，我们希望模型更加关注的是“机器”而不是“学习”

![Attention Model](https://raw.githubusercontent.com/MixLabPro/2019/master/source/uploads/2019/03/19/1.png)

![Attention Model](https://raw.githubusercontent.com/MixLabPro/2019/master/source/uploads/2019/03/19/2.png)

![Attention Model](https://raw.githubusercontent.com/MixLabPro/2019/master/source/uploads/2019/03/19/3.png)

$$
\begin{aligned} u_{t}^{i} &=v^{T} \tanh \left(W_{1} h^{i}+W_{2} Z^{t}\right) \\ \alpha_{t}^{i} &=\operatorname{softmax}\left(u_{t}^{i}\right) \\ c^{t} &=\sum_{i} \alpha_{t}^{i} h^{i} \end{aligned}
$$

# HAN的原理（Hierarchical Attention Networks）

网络结构 包括五层网络：

* 词向量编码（GRU）
* 词向量Attention层
* 句子向量编码（GRU）
* 句子向量Attention层
* softmax输出层

HAN模型结构

![HAN模型结构](https://raw.githubusercontent.com/MixLabPro/2019/master/source/uploads/2019/03/19/4.png)

HAN模型就是分层次的利用注意力机制来构建文本向量表示的方法。

文本由句子构成，句子由词构成，HAN模型对应这个结构分层的来构建文本向量表达；

文本中不同句子对文本的主旨影响程度不同，一个句子中不同的词语对句子主旨的影响程度也不同，因此HAN在词语层面和句子层面分别添加了注意力机制；

分层的注意力机制还有一个好处，可以直观的看出用这个模型构建文本表示时各个句子和单词的重要程度，增强了可解释性。

这篇论文里面使用双向GRU来构建句子表示和文本表示，以句子为例，得到循环神经网络中每个单元的输出后利用注意力机制整合得到句子向量表示（不使用attention时，一般会使用MAX或AVE），过程如下：

$$
\begin{aligned} u_{i t} &=\tanh \left(W_{w} h_{i t}+b_{w}\right) \\ \alpha_{i t} &=\frac{\exp \left(u_{i t}^{\top} u_{w}\right)}{\sum_{t} \exp \left(u_{i t}^{\top} u_{w}\right)} \\ s_{i} &=\sum_{t} \alpha_{i t} h_{i t} \end{aligned}
$$

# 利用Attention模型进行文本分类

# 参考

[浅谈Attention-based Model【原理篇】](http://blog.csdn.net/wuzqchom/article/details/75792501)
[Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)