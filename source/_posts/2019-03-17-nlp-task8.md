---
title: "任务八-循环神经网络"
categories: "NLP实践"
tags:
  - ML
  - NLP
comments: true
date: 2019-03-17 20:08:39
---

# 递归神经网络

递归神经网络（RNN）是神经网络的一种。单纯的RNN因为无法处理随着递归，权重指数级爆炸或梯度消失的问题（Vanishing gradient problem），难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。

时间递归神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。手写识别是最早成功利用RNN的研究结果。

<!--more-->

# RNN的结构

一个简单的循环神经网络，它由输入层、一个隐藏层和一个输出层组成。

![RNN结构](https://raw.githubusercontent.com/MixLabPro/2019/master/source/uploads/2019/03/17/1.jpg)

![RNN时间线展开图](https://raw.githubusercontent.com/MixLabPro/2019/master/source/uploads/2019/03/17/2.jpg)

![t 时刻具体图](https://raw.githubusercontent.com/MixLabPro/2019/master/source/uploads/2019/03/17/3.jpg)

这个网络在t时刻接收到输入 $$x_{t}$$ 之后，隐藏层的值是 $$s_{t}$$ ，输出值是 $$o_{t}$$ 。关键一点是， $$s_{t}$$ 的值不仅仅取决于 $$x_{t}$$ ，还取决于 $$s_{t-1}$$ 。我们可以用下面的公式来表示循环神经网络的计算方法：

$$
\begin{aligned} O_{t} &=g\left(V \cdot S_{t}\right) \\ S_{t} &=f\left(U \cdot X_{t}+W \cdot S_{t-1}\right) \end{aligned}
$$

循环神经网络的提出背景、优缺点。着重学习RNN的反向传播、RNN出现的问题（梯度问题、长期依赖问题）、BPTT算法。

# 双向RNN

在经典的循环神经网络中，状态的传输是从前往后单向的。然而，在有些问题中，当前时刻的输出不仅和之前的状态有关系，也和之后的状态相关。这时就需要双向RNN（BiRNN）来解决这类问题。例如预测一个语句中缺失的单词不仅需要根据前文来判断，也需要根据后面的内容，这时双向RNN就可以发挥它的作用。

双向RNN是由两个RNN上下叠加在一起组成的。输出由这两个RNN的状态共同决定。

![双向RNN](https://raw.githubusercontent.com/MixLabPro/2019/master/source/uploads/2019/03/17/4.png)

从上图可以看出，双向RNN的主题结构就是两个单向RNN的结合。在每一个时刻t，输入会同时提供给这两个方向相反的RNN，而输出则是由这两个单向RNN共同决定（可以拼接或者求和等）。

同样地，将双向RNN中的RNN替换成LSTM或者GRU结构，则组成了BiLSTM和BiGRU。

# LSTM、GRU的结构、提出背景、优缺点

# 针对梯度消失（LSTM等其他门控RNN）、梯度爆炸（梯度截断）的解决方案

# Memory Network

Memory Networks (2015)
对应论文：Memory Networks (2015)

Memory Networks 提出的基本动机是我们需要 长期记忆（long-term memory）来保存问答的知识或者聊天的语境信息，而现有的 RNN 在长期记忆中表现并没有那么好。

## 组件

4 个重要组件：

1. I: input feature map
把输入映射为特征向量（input -> feature representation）
通常以句子为单位，一个句子对应一个向量
1. G: generalization
使用新的输入数据更新 memories
1. O: output
给定新的输入和现有的 memory state，在特征空间里产生输出
类比 attention RNN decoder 产生 outputs/logits
1. R: response
将输出转化为自然语言

## 流程

![Memory Network](https://raw.githubusercontent.com/MixLabPro/2019/master/source/uploads/2019/03/17/5.png)

# Text-RNN的原理

# 利用Text-RNN模型来进行文本分类

# Recurrent Convolutional Neural Networks（RCNN）原理

[Rich feature hierarchies for accurate oject detection and semantic segmentation](https://arxiv.org/abs/1311.2524)

* R-CNN 采用 AlexNet
* R-CNN 采用 Selective Search 技术生成 Region Proposal.
* R-CNN 在 ImageNet 上先进行预训练，然后利用成熟的权重参数在 PASCAL VOC 数据集上进行 fine-tune
* R-CNN 用 CNN 抽取特征，然后用一系列的的 SVM 做类别预测。
* R-CNN 的 bbox 位置回归基于 DPM 的灵感，自己训练了一个线性回归模型。
* R-CNN 的语义分割采用 CPMC 生成 Region

# 利用RCNN模型来进行文本分类

# 参考

[递归神经网络 - 维基百科](https://zh.wikipedia.org/wiki/递归神经网络)
[一文搞懂RNN（循环神经网络）基础篇 - 知乎](https://zhuanlan.zhihu.com/p/30844905)
[深度学习笔记——RNN（LSTM、GRU、双向RNN）学习总结](https://blog.csdn.net/mpk_no1/article/details/72875185)
[论文笔记 - Memory Networks 系列 - 知乎](https://zhuanlan.zhihu.com/p/32257642)
[【深度学习】R-CNN 论文解读及个人理解](https://blog.csdn.net/briblue/article/details/82012575)
[一份详细的LSTM和GRU图解 -ATYUN](https://www.atyun.com/30234.html)
[Tensorflow实战(1): 实现深层循环神经网络 - 知乎](https://zhuanlan.zhihu.com/p/37070414)
[从LSTM到Seq2Seq-大数据算法](https://x-algo.cn/index.php/2017/01/13/1609/)
[GitHub - airalcorn2/Recurrent-Convolutional-Neural-Network-Text-Classifier](https://github.com/airalcorn2/Recurrent-Convolutional-Neural-Network-Text-Classifier)
[GitHub - zhangfazhan/TextRCNN: TextRCNN 文本分类](https://github.com/zhangfazhan/TextRCNN)
[GitHub - roomylee/rcnn-text-classification: Tensorflow Implementation of "Recurrent Convolutional Neural Network for Text Classification" (AAAI 2015)](https://github.com/roomylee/rcnn-text-classification)